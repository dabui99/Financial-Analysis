{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d0fd962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22d8c072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "\n",
    "    headers = {\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Cache-Control': 'max-age=0',\n",
    "        'Connection': 'close',\n",
    "        'DNT': '1', \n",
    "        'Pragma': 'no-cache',\n",
    "        'Referrer': 'https://google.com',\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    return requests.get(url, headers=headers)\n",
    "\n",
    "def parse_rows(table_rows):\n",
    "    parsed_rows = []\n",
    "\n",
    "    for table_row in table_rows:\n",
    "        parsed_row = []\n",
    "        el = table_row.xpath(\"./div\")\n",
    "\n",
    "        none_count = 0\n",
    "\n",
    "        for rs in el:\n",
    "            try:\n",
    "                (text,) = rs.xpath('.//span/text()[1]')\n",
    "                parsed_row.append(text)\n",
    "            except ValueError:\n",
    "                parsed_row.append(np.NaN)\n",
    "                none_count += 1\n",
    "\n",
    "        if (none_count < 4):\n",
    "            parsed_rows.append(parsed_row)\n",
    "            \n",
    "    return pd.DataFrame(parsed_rows)\n",
    "\n",
    "def clean_data(df):\n",
    "    df = df.set_index(0)\n",
    "    df = df.transpose()\n",
    "    \n",
    "    # Rename the \"Breakdown\" column to \"Date\"\n",
    "    cols = list(df.columns)\n",
    "    cols[0] = 'Date'\n",
    "    df = df.set_axis(cols, axis='columns', inplace=False)\n",
    "    \n",
    "    numeric_columns = list(df.columns)[1::] \n",
    "\n",
    "    for column_index in range(1, len(df.columns)):\n",
    "        df.iloc[:,column_index] = df.iloc[:,column_index].str.replace(',', '') \n",
    "        df.iloc[:,column_index] = df.iloc[:,column_index].astype(np.float64)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def scrape_table(url):\n",
    "    # Fetch the page that we're going to parse\n",
    "    page = get_page(url);\n",
    "\n",
    "    # Parse the page with LXML, so that we can start doing some XPATH queries\n",
    "    # to extract the data that we want\n",
    "    tree = html.fromstring(page.content)\n",
    "\n",
    "    # Fetch all div elements which have class 'D(tbr)'\n",
    "    table_rows = tree.xpath(\"//div[contains(@class, 'D(tbr)')]\")\n",
    "    \n",
    "    # Ensure that some table rows are found; if none are found, then it's possible\n",
    "    # that Yahoo Finance has changed their page layout, or have detected\n",
    "    # that you're scraping the page.\n",
    "    assert len(table_rows) > 0\n",
    "    \n",
    "    df = parse_rows(table_rows)\n",
    "    df = clean_data(df)\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aedae90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(symbol):\n",
    "    print('Attempting to scrape data for ' + symbol)\n",
    "\n",
    "    df_balance_sheet = scrape_table('https://finance.yahoo.com/quote/' + symbol + '/balance-sheet?p=' + symbol)\n",
    "    df_balance_sheet = df_balance_sheet.set_index('Date')\n",
    "\n",
    "    df_income_statement = scrape_table('https://finance.yahoo.com/quote/' + symbol + '/financials?p=' + symbol)\n",
    "    df_income_statement = df_income_statement.set_index('Date')\n",
    "    \n",
    "    df_cash_flow = scrape_table('https://finance.yahoo.com/quote/' + symbol + '/cash-flow?p=' + symbol)\n",
    "    df_cash_flow = df_cash_flow.set_index('Date')\n",
    "    \n",
    "    df_joined = df_balance_sheet \\\n",
    "        .join(df_income_statement, on='Date', how='outer', rsuffix=' - Income Statement') \\\n",
    "        .join(df_cash_flow, on='Date', how='outer', rsuffix=' - Cash Flow') \\\n",
    "        .dropna(axis=1, how='all') \\\n",
    "        .reset_index()\n",
    "            \n",
    "    df_joined.insert(1, 'Symbol', symbol)\n",
    "    \n",
    "    return df_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc55b165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_multi(symbols):\n",
    "    joint = [scrape(df) for df in symbols]\n",
    "    return pd.concat(joint,sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f9cf167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to scrape data for AAPL\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'html' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/50/vtqyjxz933jd0jlr9b23t69r0000gn/T/ipykernel_1659/621268157.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msymbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'AAPL'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FB'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_combined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/50/vtqyjxz933jd0jlr9b23t69r0000gn/T/ipykernel_1659/32126275.py\u001b[0m in \u001b[0;36mscrape_multi\u001b[0;34m(symbols)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscrape_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mjoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mscrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msymbols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/50/vtqyjxz933jd0jlr9b23t69r0000gn/T/ipykernel_1659/32126275.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscrape_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mjoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mscrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msymbols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/50/vtqyjxz933jd0jlr9b23t69r0000gn/T/ipykernel_1659/3543038032.py\u001b[0m in \u001b[0;36mscrape\u001b[0;34m(symbol)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempting to scrape data for '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msymbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdf_balance_sheet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://finance.yahoo.com/quote/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msymbol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/balance-sheet?p='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msymbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdf_balance_sheet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_balance_sheet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/50/vtqyjxz933jd0jlr9b23t69r0000gn/T/ipykernel_1659/1257019955.py\u001b[0m in \u001b[0;36mscrape_table\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Parse the page with LXML, so that we can start doing some XPATH queries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# to extract the data that we want\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# Fetch all div elements which have class 'D(tbr)'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'html' is not defined"
     ]
    }
   ],
   "source": [
    "symbols = ['AAPL', 'FB']\n",
    "df_combined = scrape_multi(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebbf748d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_combined' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/50/vtqyjxz933jd0jlr9b23t69r0000gn/T/ipykernel_1659/1543012813.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_combined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_combined' is not defined"
     ]
    }
   ],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "152d4c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to scrape data for nvda\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'html' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/50/vtqyjxz933jd0jlr9b23t69r0000gn/T/ipykernel_1659/1156375481.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nvda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/50/vtqyjxz933jd0jlr9b23t69r0000gn/T/ipykernel_1659/3543038032.py\u001b[0m in \u001b[0;36mscrape\u001b[0;34m(symbol)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempting to scrape data for '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msymbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdf_balance_sheet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://finance.yahoo.com/quote/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msymbol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/balance-sheet?p='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msymbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdf_balance_sheet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_balance_sheet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/50/vtqyjxz933jd0jlr9b23t69r0000gn/T/ipykernel_1659/1257019955.py\u001b[0m in \u001b[0;36mscrape_table\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Parse the page with LXML, so that we can start doing some XPATH queries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# to extract the data that we want\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# Fetch all div elements which have class 'D(tbr)'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'html' is not defined"
     ]
    }
   ],
   "source": [
    "scrape('nvda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d4c883",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
